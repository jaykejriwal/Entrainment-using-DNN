{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2ff7d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1 Pre-process dataset \n",
    "#Remove spaces from files\n",
    "import os\n",
    "PATH=r\"D:\\Jay\\VACC_all\\Ferige_Praat\"\n",
    "files = os.listdir(PATH)\n",
    "for root, dirs, files in os.walk(PATH):\n",
    "    for i in dirs:\n",
    "        required=i.replace(\" \", \"\")\n",
    "        if(required != i):\n",
    "            os.replace(os.path.join(PATH, i),os.path.join(PATH, required)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70863bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2 Program to merge utterance and time file\n",
    "#1 folder 20171129Ccx doesnt work for this program so remove\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import os.path\n",
    "import pandas as pd\n",
    "list_of_files = sorted(glob.glob(r'D:\\Jay\\VACC_all\\Ferige_Praat\\**\\*.txt',recursive=True)) \n",
    "\n",
    "for i in range(0,len(list_of_files),2):\n",
    "    filename = list_of_files[i]\n",
    "    print(filename)\n",
    "    filename1 = list_of_files[i+1]\n",
    "    print(filename1)\n",
    "    out_name=filename1+\"turns_revised.txt\"\n",
    "    csv_input = pd.read_csv(filename,delimiter='\\t', usecols=[0,1,2], names=['Start','Stop','Speaker'],header=None)\n",
    "    csv_input[['Start','Stop']] = csv_input[['Start','Stop']].astype(str)\n",
    "    csv_input1 = pd.read_csv(filename1,delimiter='\\t', usecols=[0,1,2], names=['Start1','Stop1','Utterance'],header=None)\n",
    "    csv_input1[['Start1','Stop1']] = csv_input1[['Start1','Stop1']].astype(str)\n",
    "    f = pd.merge(left=csv_input, right=csv_input1, left_on=['Start','Stop'], right_on=['Start1','Stop1'])\n",
    "    f.drop(['Start1', 'Stop1'], axis=1, inplace=True)\n",
    "    f = f[['Start','Stop','Speaker','Utterance']]\n",
    "    f[['Start','Stop']] = f[['Start','Stop']].astype(str)\n",
    "    f['obj1_count'] = (f['Speaker'].ne(f['Speaker'].shift())).cumsum()\n",
    "    df=f.groupby('obj1_count').agg(lambda x: ' '.join(x))\n",
    "    df['Start'] = df['Start'].map(lambda x: x.split(\" \")[0])\n",
    "    df['Stop'] = df['Stop'].map(lambda x: x.split(\" \")[-1])\n",
    "    df['Speaker'] = df['Speaker'].map(lambda x: x.split(\" \")[-1])\n",
    "    if df.iloc[-1]['Speaker'] == df.iloc[0]['Speaker']:\n",
    "        df1=df.drop(df.index[len(df)-1])\n",
    "        df1.to_csv(out_name, index=False,header=False,sep='\\t')\n",
    "        del df1\n",
    "    else:\n",
    "        df.to_csv(out_name, index=False,header=False,sep='\\t')\n",
    "    del csv_input,csv_input1,f,df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b255d936",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3 Renaming files with parent folder name\n",
    "import os\n",
    "Parent = r'D:\\Jay\\VACC_all\\Ferige_Praat'\n",
    "for root, dirs, files in os.walk(Parent):\n",
    "    if not files:\n",
    "        continue\n",
    "    prefix = os.path.basename(root)\n",
    "    for f in files:\n",
    "        os.rename(os.path.join(root, f), os.path.join(root, \"{}_{}\".format(prefix, f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c26848d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4 Remove files which are not revised\n",
    "import glob\n",
    "import os\n",
    "for f in glob.glob(r\"D:\\Jay\\VACC_all\\Ferige_Praat\\**\\*.txt\"): # find all csv files\n",
    "    if not f.endswith(\"revised.txt\"):\n",
    "        os.remove(f)  # if file name ends in 0.csv, delete it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce2c1139",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5 Rename revised files with underscore\n",
    "import os\n",
    "Parent = r'C:\\Users\\kejri\\OneDrive\\Desktop\\Jay\\VACC_all\\Ferige_Praat'\n",
    "for root, dirs, files in os.walk(Parent):\n",
    "    if not files:\n",
    "        continue\n",
    "    prefix = os.path.basename(root)\n",
    "    for f in files:\n",
    "        os.rename(os.path.join(root, f), os.path.join(root, \"{}_{}\".format(prefix, f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6 Extract semantic embeddings\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "import csv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sen_w_feats = []\n",
    "sentence_embeddings = []\n",
    "\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading Transformer...')\n",
    "model = SentenceTransformer('T-Systems-onsite/cross-en-de-roberta-sentence-transformer')\n",
    "\n",
    "list_of_files = glob.glob(r'D:\\Jay\\VACC_all\\Ferige_Praat\\**\\*.txt',recursive=True) \n",
    "output_path = r'D:\\Jay\\DNN\\VACC\\Embeddings\\Text_semantic'\n",
    "\n",
    "for file_name in list_of_files:\n",
    "    out_name= os.path.join(output_path, os.path.basename(file_name))\n",
    "    csv_input = pd.read_csv(file_name, usecols=[3], names=['utterance'],delimiter='\\t',header=None)\n",
    "    for index, row in csv_input.iterrows():\n",
    "        sen_w_feats.append(row[\"utterance\"])\n",
    "        \n",
    "    #Convert sentence to list\n",
    "    sentence_embeddings = model.encode(sen_w_feats)\n",
    "    sentence_vectors1=sentence_embeddings.tolist()\n",
    "\n",
    "    #Merge consecutive utterance of Speaker A and B\n",
    "    out = reduce(lambda x, y: x+y, sentence_vectors1)\n",
    "\n",
    "    #Each consecutive utterance is of size 1536 i.e 768 for each utterance\n",
    "    chunks = [out[x:x+1536] for x in range(0, len(out)-768, 768)]\n",
    "\n",
    "    #Convert list to array\n",
    "    arr = np.asarray(chunks)\n",
    "    with open(out_name, 'w') as fcsv:\n",
    "        writer = csv.writer(fcsv)\n",
    "        writer.writerows(arr)\n",
    "    sen_w_feats = []\n",
    "    sentence_embeddings = []\n",
    "    sentence_vectors1=None\n",
    "    arr=None\n",
    "    model_output=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 7 Create h5 files from embeddings extracted from previous step\n",
    "# The h5 files are needed for training DNN models\n",
    "import csv\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import random\n",
    "import pdb\n",
    "\n",
    "SEED=448\n",
    "frac_train = 1.0\n",
    "\n",
    "\n",
    "\n",
    "# Create h5 files\n",
    "\n",
    "#Input path\n",
    "sessList= sorted(glob.glob(r'D:\\Jay\\DNN\\VACC\\Embeddings\\Text_semantic\\*.txt',recursive=True))\n",
    "\n",
    "num_files_all = len(sessList)\n",
    "num_files_train = int(np.ceil((frac_train*num_files_all)))\n",
    "\n",
    "sessTrain = sessList[:num_files_train]\n",
    "\n",
    "# Create Train Data file\n",
    "\n",
    "X_train =np.array([])\n",
    "X_train = np.empty(shape=(0, 0), dtype='float64' )\n",
    "for sess_file in sessTrain:\n",
    "    df_i = pd.read_csv(sess_file)\n",
    "    xx=np.array(df_i)\n",
    "    X_train=np.vstack([X_train, xx]) if X_train.size else xx\n",
    "\n",
    "\n",
    "X_train = X_train.astype('float64')\n",
    "#Output path\n",
    "hf = h5py.File(r'D:\\Jay\\DNN\\VACC\\h5\\semantic\\train_nonorm.h5', 'w')\n",
    "hf.create_dataset('textdataset', data=X_train)\n",
    "hf.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1952fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 8 Create two groups\n",
    "#Program to split each h5 data into two groups\n",
    "#Even group represents turns spoken by speaker followed by Alexa\n",
    "#Odd group represents turns spoken by Alexa followed by a speaker\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Input path\n",
    "path= r'D:\\Jay\\DNN\\VACC\\h5\\semantic\\train_nonorm.h5'\n",
    "#Output path\n",
    "new_path = r'D:\\Jay\\DNN\\VACC\\h5\\semantic\\train_nonorm_even.h5'\n",
    "new_path1 = r'D:\\Jay\\DNN\\VACC\\h5\\semantic\\train_nonorm_odd.h5'\n",
    "\n",
    "with h5py.File(path, 'r') as f:\n",
    "   data_set = f['textdataset']\n",
    "   new_data_even = data_set[::2]\n",
    "   new_data_odd = data_set[1::2]\n",
    "\n",
    "with h5py.File(new_path, 'w') as f:\n",
    "   f.create_dataset('textdataset', data=new_data_even)\n",
    "\n",
    "with h5py.File(new_path1, 'w') as f:\n",
    "   f.create_dataset('textdataset', data=new_data_odd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "43e249bda334b009252ca11161b502852fd620d93fbb243f1a14cfe6570157ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
