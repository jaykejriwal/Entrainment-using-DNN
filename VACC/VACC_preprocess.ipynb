{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2ff7d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove spaces\n",
    "import os\n",
    "PATH=r\"D:\\Jay\\VACC_all\\Ferige_Praat\"\n",
    "files = os.listdir(PATH)\n",
    "for root, dirs, files in os.walk(PATH):\n",
    "    for i in dirs:\n",
    "        required=i.replace(\" \", \"\")\n",
    "        if(required != i):\n",
    "            os.replace(os.path.join(PATH, i),os.path.join(PATH, required)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70863bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171121A\\Calendar_01 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171121A\\Calendar_01.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171121A\\Quiz_01 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171121A\\Quiz_01.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171121B\\Calendar_01 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171121B\\Calendar_01.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171121B\\Quiz_01 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171121B\\Quiz_01.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171121C\\Calendar_01 (1).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171121C\\Calendar_01.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171121C\\Quiz_01 (1).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171121C\\Quiz_01.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171122A\\Calendar_01(2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171122A\\Calendar_01.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171122A\\Quiz_01 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171122A\\Quiz_01.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171122B\\Calendar_01 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171122B\\Calendar_01 .txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171122B\\Quiz_01 (1).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171122B\\Quiz_01 .txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171122C\\Calendar_01 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171122C\\Calendar_01.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171122C\\Quiz_01 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171122C\\Quiz_01.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171123A\\Calendar_01 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171123A\\Calendar_01.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171123A\\Quiz_01 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171123A\\Quiz_01.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171123Bct\\Calendar_02 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171123Bct\\Calendar_02.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171123Bct\\Quiz_02 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171123Bct\\Quiz_02.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171123Dct\\Calendar_02 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171123Dct\\Calendar_02.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171123Dct\\Quiz_02 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171123Dct\\Quiz_02.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171123Ect\\Calendar_02 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171123Ect\\Calendar_02.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171123Ect\\Quiz_02 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171123Ect\\Quiz_02.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171123Fct\\Calendar_02 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171123Fct\\Calendar_02.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171123Fct\\Quiz_02 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171123Fct\\Quiz_02.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171127Acx\\Calendar_01 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171127Acx\\Calendar_01.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171127Acx\\Quiz_02 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171127Acx\\Quiz_02.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171127Bcx\\Calendar_01 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171127Bcx\\Calendar_01.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171127Bcx\\Quiz_02 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171127Bcx\\Quiz_02.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171127Ccx\\Calendar_01 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171127Ccx\\Calendar_01.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171127Ccx\\Quiz_02 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171127Ccx\\Quiz_02.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171128Acx\\Calendar_01 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171128Acx\\Calendar_01.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171128Acx\\Quiz_02 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171128Acx\\Quiz_02.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171128Bcx\\Calendar_01 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171128Bcx\\Calendar_01.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171128Bcx\\Quiz_02 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171128Bcx\\Quiz_02.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171129Acx\\Calendar_01 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171129Acx\\Calendar_01.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171129Acx\\Quiz_02 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171129Acx\\Quiz_02.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171129Bcx\\Calendar_02 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171129Bcx\\Calendar_02.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171129Bcx\\Quiz_01 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171129Bcx\\Quiz_01.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171129Ccx\\Calendar_02 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171129Ccx\\Calendar_02.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kejri\\AppData\\Local\\Temp\\ipykernel_15092\\3116687055.py:24: FutureWarning: ['Utterance'] did not aggregate successfully. If any error is raised this will raise in a future version of pandas. Drop these columns/ops to avoid this warning.\n",
      "  df=f.groupby('obj1_count').agg(lambda x: ' '.join(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171129Ccx\\Quiz_01 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171129Ccx\\Quiz_01.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171129Dcx\\Calendar_02 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171129Dcx\\Calendar_02.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171129Dcx\\Quiz_01 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171129Dcx\\Quiz_01.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171130Acx\\Calendar_02 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171130Acx\\Calendar_02.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171130Acx\\Quiz_01 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171130Acx\\Quiz_01.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171130Bcx\\Calendar_02 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171130Bcx\\Calendar_02.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171130Bcx\\Quiz_01 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171130Bcx\\Quiz_01.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171130Ccx\\Calendar_02 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171130Ccx\\Calendar_02.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171130Ccx\\Quiz_01 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171130Ccx\\Quiz_01.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171201A\\Calendar_01 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171201A\\Calendar_01.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171201A\\Quiz_01 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171201A\\Quiz_01.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171201Bcx\\Calendar_01 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171201Bcx\\Calendar_01.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171201Bcx\\Quiz_02 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171201Bcx\\Quiz_02.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171208Acx\\Calendar_02 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171208Acx\\Calendar_02.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171208Acx\\Quiz_01 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171208Acx\\Quiz_01.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171208Bcx\\Calendar_01 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171208Bcx\\Calendar_01.txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171208Bcx\\Quiz_02 (2).txt\n",
      "D:\\Jay\\VACC_all\\Ferige_Praat\\20171208Bcx\\Quiz_02.txt\n"
     ]
    }
   ],
   "source": [
    "#Program to merge utterance and time file\n",
    "#1 folder 20171129Ccx doesnt work for this program so remove\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import os.path\n",
    "import pandas as pd\n",
    "list_of_files = sorted(glob.glob(r'D:\\Jay\\VACC_all\\Ferige_Praat\\**\\*.txt',recursive=True)) \n",
    "\n",
    "for i in range(0,len(list_of_files),2):\n",
    "    filename = list_of_files[i]\n",
    "    print(filename)\n",
    "    filename1 = list_of_files[i+1]\n",
    "    print(filename1)\n",
    "    out_name=filename1+\"turns_revised.txt\"\n",
    "    csv_input = pd.read_csv(filename,delimiter='\\t', usecols=[0,1,2], names=['Start','Stop','Speaker'],header=None)\n",
    "    csv_input[['Start','Stop']] = csv_input[['Start','Stop']].astype(str)\n",
    "    csv_input1 = pd.read_csv(filename1,delimiter='\\t', usecols=[0,1,2], names=['Start1','Stop1','Utterance'],header=None)\n",
    "    csv_input1[['Start1','Stop1']] = csv_input1[['Start1','Stop1']].astype(str)\n",
    "    f = pd.merge(left=csv_input, right=csv_input1, left_on=['Start','Stop'], right_on=['Start1','Stop1'])\n",
    "    f.drop(['Start1', 'Stop1'], axis=1, inplace=True)\n",
    "    f = f[['Start','Stop','Speaker','Utterance']]\n",
    "    f[['Start','Stop']] = f[['Start','Stop']].astype(str)\n",
    "    f['obj1_count'] = (f['Speaker'].ne(f['Speaker'].shift())).cumsum()\n",
    "    df=f.groupby('obj1_count').agg(lambda x: ' '.join(x))\n",
    "    df['Start'] = df['Start'].map(lambda x: x.split(\" \")[0])\n",
    "    df['Stop'] = df['Stop'].map(lambda x: x.split(\" \")[-1])\n",
    "    df['Speaker'] = df['Speaker'].map(lambda x: x.split(\" \")[-1])\n",
    "    if df.iloc[-1]['Speaker'] == df.iloc[0]['Speaker']:\n",
    "        df1=df.drop(df.index[len(df)-1])\n",
    "        df1.to_csv(out_name, index=False,header=False,sep='\\t')\n",
    "        del df1\n",
    "    else:\n",
    "        df.to_csv(out_name, index=False,header=False,sep='\\t')\n",
    "    del csv_input,csv_input1,f,df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b255d936",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming files with parent folder name\n",
    "import os\n",
    "Parent = r'D:\\Jay\\VACC_all\\Ferige_Praat'\n",
    "for root, dirs, files in os.walk(Parent):\n",
    "    if not files:\n",
    "        continue\n",
    "    prefix = os.path.basename(root)\n",
    "    for f in files:\n",
    "        os.rename(os.path.join(root, f), os.path.join(root, \"{}_{}\".format(prefix, f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c26848d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "for f in glob.glob(r\"D:\\Jay\\VACC_all\\Ferige_Praat\\**\\*.txt\"): # find all csv files\n",
    "    if not f.endswith(\"revised.txt\"):\n",
    "        os.remove(f)  # if file name ends in 0.csv, delete it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce2c1139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "Parent = r'C:\\Users\\kejri\\OneDrive\\Desktop\\Jay\\VACC_all\\Ferige_Praat'\n",
    "for root, dirs, files in os.walk(Parent):\n",
    "    if not files:\n",
    "        continue\n",
    "    prefix = os.path.basename(root)\n",
    "    for f in files:\n",
    "        os.rename(os.path.join(root, f), os.path.join(root, \"{}_{}\".format(prefix, f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Transformer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02018dcc7034437496cf63e506f8692f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7ceaf/.gitattributes:   0%|          | 0.00/445 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7970ba0aad784f89bbbe1b589ae284fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e177eaa7ceaf/LICENSE:   0%|          | 0.00/1.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8accab9512a54dd8828be7f4a4013f69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)77eaa7ceaf/README.md:   0%|          | 0.00/8.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc30ceb99454a488309c7557dc25f40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)eaa7ceaf/config.json:   0%|          | 0.00/541 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22da6ff34704487b99e1ee168e8b17fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6864aea1d7b64d8098b41a9449d4a809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5146262f12604a8383d862cb803a1064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/27.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b86d5c8b18d4c56a7af53cf635caf13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1298090739a84976adb0873aab1d6b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394e98ba32a84582b1b45540a5d2f340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name C:\\Users\\kejri/.cache\\torch\\sentence_transformers\\T-Systems-onsite_cross-en-de-roberta-sentence-transformer. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "#1Extract_semantic_text_embeddings.py\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "import csv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sen_w_feats = []\n",
    "sentence_embeddings = []\n",
    "\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading Transformer...')\n",
    "model = SentenceTransformer('T-Systems-onsite/cross-en-de-roberta-sentence-transformer')\n",
    "\n",
    "list_of_files = glob.glob(r'D:\\Jay\\VACC_all\\Ferige_Praat\\**\\*.txt',recursive=True) \n",
    "output_path = r'D:\\Jay\\DNN\\VACC\\Embeddings\\Text_semantic'\n",
    "\n",
    "for file_name in list_of_files:\n",
    "    out_name= os.path.join(output_path, os.path.basename(file_name))\n",
    "    csv_input = pd.read_csv(file_name, usecols=[3], names=['utterance'],delimiter='\\t',header=None)\n",
    "    for index, row in csv_input.iterrows():\n",
    "        sen_w_feats.append(row[\"utterance\"])\n",
    "        \n",
    "    #Convert sentence to list\n",
    "    sentence_embeddings = model.encode(sen_w_feats)\n",
    "    sentence_vectors1=sentence_embeddings.tolist()\n",
    "\n",
    "    #Merge consecutive utterance of Speaker A and B\n",
    "    out = reduce(lambda x, y: x+y, sentence_vectors1)\n",
    "\n",
    "    #Each consecutive utterance is of size 1536 i.e 768 for each utterance\n",
    "    chunks = [out[x:x+1536] for x in range(0, len(out)-768, 768)]\n",
    "\n",
    "    #Convert list to array\n",
    "    arr = np.asarray(chunks)\n",
    "    with open(out_name, 'w') as fcsv:\n",
    "        writer = csv.writer(fcsv)\n",
    "        writer.writerows(arr)\n",
    "    sen_w_feats = []\n",
    "    sentence_embeddings = []\n",
    "    sentence_vectors1=None\n",
    "    arr=None\n",
    "    model_output=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2create_h5data_Turn_semantic\n",
    "import csv\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import random\n",
    "import pdb\n",
    "\n",
    "SEED=448\n",
    "frac_train = 1.0\n",
    "\n",
    "\n",
    "\n",
    "# Create h5 files\n",
    "\n",
    "\n",
    "sessList= sorted(glob.glob(r'D:\\Jay\\DNN\\VACC\\Embeddings\\Text_semantic\\*.txt',recursive=True))\n",
    "\n",
    "num_files_all = len(sessList)\n",
    "num_files_train = int(np.ceil((frac_train*num_files_all)))\n",
    "\n",
    "sessTrain = sessList[:num_files_train]\n",
    "\n",
    "# Create Train Data file\n",
    "\n",
    "X_train =np.array([])\n",
    "X_train = np.empty(shape=(0, 0), dtype='float64' )\n",
    "for sess_file in sessTrain:\n",
    "    df_i = pd.read_csv(sess_file)\n",
    "    xx=np.array(df_i)\n",
    "    X_train=np.vstack([X_train, xx]) if X_train.size else xx\n",
    "\n",
    "\n",
    "X_train = X_train.astype('float64')\n",
    "hf = h5py.File(r'D:\\Jay\\DNN\\VACC\\h5\\semantic\\train_nonorm.h5', 'w')\n",
    "hf.create_dataset('textdataset', data=X_train)\n",
    "hf.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1952fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3create_even_odd_h5_semantic\n",
    "#Program to split each h5 data into two groups\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "path= r'D:\\Jay\\DNN\\VACC\\h5\\semantic\\train_nonorm.h5'\n",
    "new_path = r'D:\\Jay\\DNN\\VACC\\h5\\semantic\\train_nonorm_even.h5'\n",
    "new_path1 = r'D:\\Jay\\DNN\\VACC\\h5\\semantic\\train_nonorm_odd.h5'\n",
    "\n",
    "with h5py.File(path, 'r') as f:\n",
    "   data_set = f['textdataset']\n",
    "   new_data_even = data_set[::2]\n",
    "   new_data_odd = data_set[1::2]\n",
    "\n",
    "with h5py.File(new_path, 'w') as f:\n",
    "   f.create_dataset('textdataset', data=new_data_even)\n",
    "\n",
    "with h5py.File(new_path1, 'w') as f:\n",
    "   f.create_dataset('textdataset', data=new_data_odd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "43e249bda334b009252ca11161b502852fd620d93fbb243f1a14cfe6570157ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
