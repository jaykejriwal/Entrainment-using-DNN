{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First step \n",
    "## Download the columbia games corpus to a specific location\n",
    "## Later, execute the code in sequence \n",
    "## This program reads each words file and creates a new words file with additional information about \"speaker\" and \"filename\".\n",
    "## The program reads the .words file from each folder and creates a new words file with the extension .words_with_speaker\n",
    "## For instance, s01.cards.1.A.words will be processed, and the new file will be created on the same path with s01.cards.1.A.words_with_speaker\n",
    "\n",
    "#Program to add speaker and filename column in each word file\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#Specify the path of the dataset\n",
    "list_of_files = glob.glob(r'D:\\Jay\\columbia-games-corpus\\data\\**\\*.words',recursive=True) \n",
    "\n",
    "for file_name in list_of_files:    \n",
    "    out_name=file_name+'_with_speaker'\n",
    "    csv_input = pd.read_csv(file_name,delimiter=' ',header=None)\n",
    "    csv_input['Speaker'] = file_name[-7]\n",
    "    csv_input['filename'] = file_name\n",
    "    csv_input.to_csv(out_name, index=False,header=False,sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second step \n",
    "## The program reads each words_with_speaker file processed in the first step and combines the text files of both speakers in each session.\n",
    "## For instance, s01.cards.1.A.words_with_speaker and s01.cards.1.B.words_with_speaker will be read and combined. \n",
    "## The output will be saved as s01.objects.1.A.words_with_speaker_txt\n",
    "## The program also converts each flac file into wav file format\n",
    "## For instance, s01.cards.1.A.flac will be transformed to s01.objects.1.A.wav\n",
    "\n",
    "#Program to merge Speaker text files and convert flac files\n",
    "import os\n",
    "import fnmatch\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "#Specify path of dataset\n",
    "OLD_BASE = r'D:\\Jay\\columbia-games-corpus\\data'\n",
    "\n",
    "#Create the empty folder where ouput needs to be saved\n",
    "#Specify path of output folder\n",
    "NEW_BASE = r'D:\\Jay\\columbia-games-corpus\\jkdata'\n",
    "\n",
    "def merge_files(infiles):\n",
    "    output=os.path.basename(infiles[0])+'_txt'\n",
    "    outfiletxt = os.path.join(new_dir, output)\n",
    "    with open(outfiletxt, 'wb') as fo:\n",
    "        for infile in infiles:\n",
    "            with open(infile, 'rb') as fi:\n",
    "                fo.write(fi.read())\n",
    "\n",
    "def convert_files(infiles):\n",
    "    output_channel1=os.path.basename(infiles[0][:-5])+'.wav'\n",
    "    output_channel2=os.path.basename(infiles[1][:-5])+'.wav'\n",
    "    outfile1 = os.path.join(new_dir, output_channel1)\n",
    "    outfile2 = os.path.join(new_dir, output_channel2)\n",
    "    #Specify path of ffmpeg for converting audio files\n",
    "    ffp=r\"C:\\ffmpeg-master-latest-win64-gpl\\bin\\ffmpeg\"\n",
    "    cmd2wav1 = ffp+' -i ' + infiles[0] + ' ' + outfile1\n",
    "    cmd2wav2 = ffp+' -i ' + infiles[1] + ' ' + outfile2\n",
    "    subprocess.call(cmd2wav1, shell=True)\n",
    "    subprocess.call(cmd2wav2, shell=True)\n",
    "\n",
    "for (dirpath, dirnames, filenames) in os.walk(OLD_BASE):\n",
    "    base, tail = os.path.split(dirpath)\n",
    "    if base != OLD_BASE: continue  # Don't operate on OLD_BASE, only children directories\n",
    "\n",
    "    # Build infiles list for flac objects\n",
    "    object_flac = sorted([os.path.join(dirpath, filename) for filename in filenames if fnmatch.fnmatch(filename,\"*objects*.flac\")])\n",
    "    # Build infiles list for words objects\n",
    "    object_text = sorted([os.path.join(dirpath, filename) for filename in filenames if fnmatch.fnmatch(filename,\"*objects*.words_with_speaker\")])\n",
    "\n",
    "    # Create output directory\n",
    "    new_dir =  os.path.join(NEW_BASE, tail)\n",
    "    os.mkdir(new_dir)  # This will raise an OSError if the directory already exists    \n",
    "\n",
    "    # Merge\n",
    "    convert_files(object_flac)\n",
    "    merge_files(object_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Third step \n",
    "## The program reads each concatenated words_with_speaker_txt file processed in the second step and further processes it in the following way.\n",
    "## Each word consecutively spoken by a speaker is merged and converted into a turn.\n",
    "## Similarly, start and end times are adjusted accordingly\n",
    "## For instance, s01.objects.1.A.words_with_speaker_txt will be read and s01.objects.1.txt will be generated in same folder. \n",
    "\n",
    "#Program to extract turns\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import os.path\n",
    "import pandas as pd\n",
    "\n",
    "#Add path for output files saved in second step\n",
    "list_of_files = glob.glob(r'D:\\Jay\\columbia-games-corpus\\jkdata\\**\\*.words_with_speaker_txt',recursive=True) \n",
    "\n",
    "for file_name in list_of_files:    \n",
    "    out_name=file_name[:-24]+'txt'\n",
    "    headers= ['start','end','words','Speaker','filename']\n",
    "    csv_input = pd.read_csv(file_name,delimiter=' ')\n",
    "    csv_input.columns = headers\n",
    "    cgc2=csv_input.sort_values(['start'])    \n",
    "    cgc3=cgc2[cgc2[\"words\"].str.contains(\"#\")==False]\n",
    "    cgc3[['start', 'end']] = cgc3[['start', 'end']].astype(str)\n",
    "    #Merge consecutive turns of same speaker\n",
    "    cgc3['obj1_count'] = (cgc3['Speaker'].ne(cgc3['Speaker'].shift())).cumsum()\n",
    "    df3=cgc3.groupby('obj1_count').agg(lambda x: ' '.join(x))\n",
    "    #Adjust start and end time\n",
    "    df3['start'] = df3['start'].map(lambda x: x.split(\" \")[0])\n",
    "    df3['end'] = df3['end'].map(lambda x: x.split(\" \")[-1])\n",
    "    #Add speaker information\n",
    "    df3['Speaker'] = df3['Speaker'].map(lambda x: x.split(\" \")[-1])\n",
    "    df3['filename'] = df3['filename'].map(lambda x: x.split(\" \")[-1])\n",
    "    df3['filename'] = df3['filename'].str.replace('words','flac')\n",
    "    #Add session information\n",
    "    df3['session']=re.search('(\\d\\d)',file_name)[0]\n",
    "    if df3.iloc[-1]['Speaker'] == df3.iloc[0]['Speaker']:\n",
    "        df4=df3.drop(df3.index[len(df3)-1])\n",
    "        df4.to_csv(out_name, index=False,header=False,sep='\\t')\n",
    "        del df4\n",
    "    else:\n",
    "        df3.to_csv(out_name, index=False,header=False,sep='\\t')\n",
    "    del csv_input,cgc2,cgc3,df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fourth step \n",
    "## The program reads each processed file from previous step and extracts semantic embeddings\n",
    "##Extract semantic embeddings from files\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "import csv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sen_w_feats = []\n",
    "sentence_embeddings = []\n",
    "\n",
    "#Initialize transformer model for semantic feature extraction\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading Transformer...')\n",
    "model = SentenceTransformer('sentence-transformers/msmarco-distilbert-dot-v5')\n",
    "\n",
    "##Specify input path from where text files needs to be read\n",
    "#Specify path for files generated in third step\n",
    "list_of_files = glob.glob(r'D:\\Jay\\columbia-games-corpus\\jkdata\\**\\*.txt',recursive=True) \n",
    "\n",
    "#Specify path where semantic embeddings needs to be saved\n",
    "output_path = r'D:\\Jay\\DNN\\Columbia\\Embeddings\\Text_semantic'\n",
    "\n",
    "for file_name in list_of_files:\n",
    "    out_name= os.path.join(output_path, os.path.basename(file_name))\n",
    "    csv_input = pd.read_csv(file_name, usecols=[2], names=['utterance'],delimiter='\\t',header=None)\n",
    "    for index, row in csv_input.iterrows():\n",
    "        sen_w_feats.append(row[\"utterance\"])\n",
    "        \n",
    "    #Convert sentence to list\n",
    "    sentence_embeddings = model.encode(sen_w_feats)\n",
    "    sentence_vectors1=sentence_embeddings.tolist()\n",
    "\n",
    "    #Merge consecutive utterance of Speaker A and B\n",
    "    out = reduce(lambda x, y: x+y, sentence_vectors1)\n",
    "\n",
    "    #Each consecutive utterance is of size 1536 i.e 768 for each utterance\n",
    "    chunks = [out[x:x+1536] for x in range(0, len(out)-768, 768)]\n",
    "\n",
    "    #Convert list to array\n",
    "    arr = np.asarray(chunks)\n",
    "    with open(out_name, 'w') as fcsv:\n",
    "        writer = csv.writer(fcsv)\n",
    "        writer.writerows(arr)\n",
    "    sen_w_feats = []\n",
    "    sentence_embeddings = []\n",
    "    sentence_vectors1=None\n",
    "    arr=None\n",
    "    model_output=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fifth step \n",
    "## The program reads each processed file from previous step and converts them into h5 format\n",
    "\n",
    "import csv\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import random\n",
    "import pdb\n",
    "\n",
    "SEED=448\n",
    "frac_train = 1.0\n",
    "\n",
    "# Create h5 files\n",
    "\n",
    "##Specify input path from where embeddings files needs to be read\n",
    "#Specify path for files generated in fourth step\n",
    "sessList= sorted(glob.glob(r'D:\\Jay\\DNN\\Columbia\\Embeddings\\Text_semantic\\*.txt',recursive=True))\n",
    "\n",
    "num_files_all = len(sessList)\n",
    "num_files_train = int(np.ceil((frac_train*num_files_all)))\n",
    "\n",
    "sessTrain = sessList[:num_files_train]\n",
    "\n",
    "# Create Train Data file\n",
    "\n",
    "X_train =np.array([])\n",
    "X_train = np.empty(shape=(0, 0), dtype='float64' )\n",
    "for sess_file in sessTrain:\n",
    "    df_i = pd.read_csv(sess_file)\n",
    "    xx=np.array(df_i)\n",
    "    X_train=np.vstack([X_train, xx]) if X_train.size else xx\n",
    "\n",
    "\n",
    "X_train = X_train.astype('float64')\n",
    "\n",
    "#Specify path where h5 embeddings needs to be saved\n",
    "hf = h5py.File(r'D:\\Jay\\DNN\\Columbia\\h5\\semantic\\train_nonorm.h5', 'w')\n",
    "hf.create_dataset('textdataset', data=X_train)\n",
    "hf.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sixth step \n",
    "#The program to split each h5 data into two groups\n",
    "##The program reads h5r files generated in previous step and splits them into two groups\n",
    "##The first group (Even) has utterance spoken by speaker A followed by speaker B\n",
    "##The second group (Odd) has utterance spoken by speaker B followed by speaker A\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "##Specify input path from where h5 file needs to be read\n",
    "#Specify path for files generated in fifth step\n",
    "path= r'D:\\Jay\\DNN\\Columbia\\h5\\semantic\\train_nonorm.h5'\n",
    "\n",
    "##Specify the output paths\n",
    "new_path = r'D:\\Jay\\DNN\\Columbia\\h5\\semantic\\train_nonorm_even.h5'\n",
    "new_path1 = r'D:\\Jay\\DNN\\Columbia\\h5\\semantic\\train_nonorm_odd.h5'\n",
    "\n",
    "with h5py.File(path, 'r') as f:\n",
    "   data_set = f['textdataset']\n",
    "   new_data_even = data_set[::2]\n",
    "   new_data_odd = data_set[1::2]\n",
    "\n",
    "with h5py.File(new_path, 'w') as f:\n",
    "   f.create_dataset('textdataset', data=new_data_even)\n",
    "\n",
    "with h5py.File(new_path1, 'w') as f:\n",
    "   f.create_dataset('textdataset', data=new_data_odd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "43e249bda334b009252ca11161b502852fd620d93fbb243f1a14cfe6570157ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
